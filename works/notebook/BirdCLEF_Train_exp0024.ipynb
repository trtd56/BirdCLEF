{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BirdCLEF_Train_exp0024.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2kY0qCDAS0-"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7LAfWzBAd-a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsGMqFzVfNWt"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TueNq39B76ko"
      },
      "source": [
        "!pip install timm torchaudio evaluations wandb audiomentations acoustics google-cloud-secret-manager torchlibrosa > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XDrF2CGffM7"
      },
      "source": [
        "from google.cloud import secretmanager\n",
        "\n",
        "def access_secret(project_id, secret_name, version='latest'):\n",
        "    client = secretmanager.SecretManagerServiceClient()\n",
        "    name = client.secret_version_path(project_id, secret_name, version)\n",
        "    response = client.access_secret_version(request={\"name\":name})\n",
        "    payload = response.payload.data.decode(\"UTF-8\")\n",
        "    return payload\n",
        "\n",
        "PROJECT_ID = \"cyberagent-312\"\n",
        "SECRET_NAME  = \"wandb\"\n",
        "wandb_key = access_secret(PROJECT_ID, SECRET_NAME)\n",
        "\n",
        "!wandb login {wandb_key}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6LRDkJSSskb"
      },
      "source": [
        "!mkdir -p birdclef-2021\n",
        "\n",
        "!cp -r ./drive/MyDrive/Study/BirdCLEF/input/birdclef-2021/train_soundscapes birdclef-2021\n",
        "!cp -r ./drive/MyDrive/Study/BirdCLEF/input/birdclef-2021/train_metadata.csv birdclef-2021\n",
        "!cp -r ./drive/MyDrive/Study/BirdCLEF/input/birdclef-2021/train_soundscape_labels.csv birdclef-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JssPm9jrAlV0"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p .kaggle\n",
        "!cp \"./drive/My Drive/Study/config/kaggle.json\" .kaggle/\n",
        "!chmod 600 .kaggle/kaggle.json\n",
        "!mv .kaggle /root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra1HjFKF6Elq"
      },
      "source": [
        "%%time\n",
        "# 3分くらい\n",
        "\n",
        "# 500-400\n",
        "#!kaggle datasets download takamichitoda/birdclef-split-audio-frequency-500400\n",
        "#!unzip birdclef-split-audio-frequency-500400.zip -d birdclef-2021 > /dev/null\n",
        "#!rm birdclef-split-audio-frequency-500400.zip \n",
        "\n",
        "\n",
        "# 400-300\n",
        "#!kaggle datasets download takamichitoda/birdclef-split-audio-by-label-frequency-400300\n",
        "#!unzip birdclef-split-audio-by-label-frequency-400300.zip -d birdclef-2021 > /dev/null\n",
        "#!rm birdclef-split-audio-by-label-frequency-400300.zip\n",
        "\n",
        "# 300-250\n",
        "!kaggle datasets download takamichitoda/birdclef-split-audio-frequency-300250\n",
        "!unzip birdclef-split-audio-frequency-300250.zip -d birdclef-2021 > /dev/null\n",
        "!rm birdclef-split-audio-frequency-300250.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GHfTaGo7RAy"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import psutil\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import albumentations as A\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import wandb\n",
        "import timm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
        "from IPython.display import Audio\n",
        "from audiomentations import Compose, AddGaussianNoise, AddBackgroundNoise, AddGaussianSNR, AddShortNoises, Gain\n",
        "import acoustics\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from evaluations.kaggle_2020 import row_wise_micro_averaged_f1_score\n",
        "from torchlibrosa.augmentation import DropStripes\n",
        "from pathlib import Path\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChM3jO_qhDYg"
      },
      "source": [
        "\"\"\"def load_mel_spec(data):\n",
        "    mel_spec = librosa.feature.melspectrogram(y=data, \n",
        "                                              sr=32000, \n",
        "                                              n_fft=2048, \n",
        "                                              n_mels=128, \n",
        "                                              fmin=20, \n",
        "                                              fmax=16000)\n",
        "    mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n",
        "    return mel_spec\n",
        "\n",
        "wave, sr = sf.read(f\"birdclef-2021/daejun/XC269060.ogg\")\n",
        "spec_x = load_mel_spec(wave)\n",
        "\n",
        "idx = 0\n",
        "plt.imshow(spec_x[:, 313*idx:313*(idx+1)])\n",
        "Audio(wave, rate=32000)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTW9NsOx7-op"
      },
      "source": [
        "class config:\n",
        "    EXP_NUM = \"0024\"\n",
        "    EXP_NAME = \"frame_max_loss\"\n",
        "    # data setting\n",
        "    INPUT_ROOT = \"/content/birdclef-2021\"\n",
        "    WORK_ROOT = \"/content\"\n",
        "    OUTPUT_ROOT = \"/content/drive/MyDrive/Study/BirdCLEF/output\"\n",
        "    NOISE_ROOT = \"/content/noise/\"\n",
        "    LABEL_FREQ = \"300-250\"\n",
        "    # audio setting\n",
        "    SAMPLE_RATE = 32000\n",
        "    FMIN = 20\n",
        "    FMAX = 16000\n",
        "    N_FFT = 2048\n",
        "    SPEC_HEIGHT = 128\n",
        "    PERIOD = 5\n",
        "    HOP_LENGTH = 512\n",
        "    # AudioAugument\n",
        "    MAX_SNR_IN_DB = 100\n",
        "    MIN_SNR_IN_DB = 3\n",
        "    # ML setting\n",
        "    SEED = 416\n",
        "    BATCH_SIZE = 64\n",
        "    MODEL_NAME = \"resnet18\"\n",
        "    LEARNING_RATE = 1e-3\n",
        "    T_MAX = 5\n",
        "    NUM_EPOCHS = 5\n",
        "    N_ACCUMULATE = 1\n",
        "    LABEL_SMOOTHING = 0.1\n",
        "    # infer setting\n",
        "    THRESHOLD = 0.5\n",
        "print(\"exp number:\", config.EXP_NUM)\n",
        "print(\"detail:\", config.EXP_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvUvLhMDaVhP"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mcv1iOGUcY"
      },
      "source": [
        "# https://github.com/karolpiczak/ESC-50\n",
        "if not os.path.exists(config.NOISE_ROOT):\n",
        "\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}\", exist_ok=True)\n",
        "\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/simple\", exist_ok=True)\n",
        "\n",
        "    brown_noise = acoustics.generator.brown(5*config.SAMPLE_RATE)\n",
        "    pink_noise = acoustics.generator.pink(5*config.SAMPLE_RATE, np.random.RandomState(config.SEED))\n",
        "\n",
        "    sf.write(f\"{config.NOISE_ROOT}/simple/brown_noise.wav\", brown_noise, samplerate=config.SAMPLE_RATE)\n",
        "    sf.write(f\"{config.NOISE_ROOT}/simple/pink_noise.wav\", pink_noise, samplerate=config.SAMPLE_RATE)\n",
        "\n",
        "    !git clone https://github.com/karolpiczak/ESC-50.git\n",
        "\n",
        "    esc50_meta_df = pd.read_csv(\"ESC-50/meta/esc50.csv\")\n",
        "    airplane_fnames = esc50_meta_df.query(\"category=='airplane'\")[\"filename\"]\n",
        "    rain_fnames = esc50_meta_df.query(\"category=='rain'\")[\"filename\"]\n",
        "    wind_fnames = esc50_meta_df.query(\"category=='wind'\")[\"filename\"]\n",
        "    insects_fnames = esc50_meta_df.query(\"category=='insects'\")[\"filename\"]\n",
        "    engine_fnames = esc50_meta_df.query(\"category=='engine'\")[\"filename\"]\n",
        "    crickets_fnames = esc50_meta_df.query(\"category=='crickets'\")[\"filename\"]\n",
        "    water_drops_fnames = esc50_meta_df.query(\"category=='water_drops'\")[\"filename\"]\n",
        "    crackling_fire_fnames = esc50_meta_df.query(\"category=='crackling_fire'\")[\"filename\"]\n",
        "    frog_fnames = esc50_meta_df.query(\"category=='frog'\")[\"filename\"]\n",
        "\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/airplane\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/rain\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/wind\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/insects\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/engine\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/crickets\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/water_drops\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/crackling_fire\", exist_ok=True)\n",
        "    os.makedirs(f\"{config.NOISE_ROOT}/frog\", exist_ok=True)\n",
        "\n",
        "    for fname in airplane_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/airplane/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in rain_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/rain/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in wind_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/wind/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in insects_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/insects/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in engine_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/engine/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in crickets_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/crickets/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in water_drops_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/water_drops/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in crackling_fire_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/crackling_fire/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)\n",
        "    for fname in frog_fnames:\n",
        "        wave, sr = sf.read(f\"ESC-50/audio/{fname}\")\n",
        "        sf.write(f\"{config.NOISE_ROOT}/frog/{fname}.wav\", wave, samplerate=config.SAMPLE_RATE)                          \n",
        "    !rm -rf ESC-50\n",
        "\n",
        "!ls {config.NOISE_ROOT}/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5QsIqBj8mh0"
      },
      "source": [
        "def arrange_wave_length(waveform):\n",
        "    effective_length = config.PERIOD * config.SAMPLE_RATE\n",
        "    input_length = waveform.shape[1]\n",
        "    if input_length > effective_length:\n",
        "        _waveform = waveform[:, :effective_length]\n",
        "    elif input_length < effective_length:\n",
        "        pad = torch.zeros((1, effective_length - input_length))\n",
        "        _waveform = torch.hstack([waveform, pad])\n",
        "    else:\n",
        "        _waveform = waveform\n",
        "    return _waveform\n",
        "\n",
        "class BirdCLEFTrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fnames, labels, mode):\n",
        "        self.fnames = fnames\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "\n",
        "        # https://github.com/iver56/audiomentations\n",
        "        self.augment = Compose([\n",
        "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "            AddGaussianSNR(min_SNR=0.001, max_SNR=1.0, p=0.5),\n",
        "            AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/simple\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/airplane\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/rain\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/wind\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/insects\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/engine\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/crickets\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/frog\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/crackling_fire\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #AddBackgroundNoise(sounds_path=f\"{config.NOISE_ROOT}/water_drops\", min_snr_in_db=config.MIN_SNR_IN_DB, max_snr_in_db=config.MAX_SNR_IN_DB, p=0.5),\n",
        "            #Gain(min_gain_in_db=-12, max_gain_in_db=12, p=0.5),\n",
        "            #AddShortNoises(config.NOISE_ROOT, p=0.5),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fnames[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        audio_path = f\"{config.INPUT_ROOT}/{label}/{fname}\"\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        waveform = arrange_wave_length(waveform)\n",
        "        waveform = waveform.numpy()[0]\n",
        "        if self.mode == \"train\":\n",
        "            waveform = self.augment(waveform, sample_rate=config.SAMPLE_RATE)\n",
        "        \n",
        "        label_ohe = torch.eye(n_labels)[label_dic[label]]\n",
        "        if self.mode == \"train\":\n",
        "            label_ohe = label_ohe * (1 - config.LABEL_SMOOTHING) + (config.LABEL_SMOOTHING/n_labels)\n",
        "        \n",
        "        return waveform, label_ohe\n",
        "\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, all_audios):\n",
        "        self.all_audios = all_audios\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.all_audios)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.all_audios[idx]\n",
        "        audio_id, site, _ = audio_path.name.split(\"_\")\n",
        "        clip, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        clips, row_ids = [], []\n",
        "        for tail_s in range(5, 605, 5):\n",
        "            head_s = tail_s - 5\n",
        "            _clip = clip[:, head_s*config.SAMPLE_RATE:tail_s*config.SAMPLE_RATE]\n",
        "            clips.append(_clip)\n",
        "            row_ids.append(f\"{audio_id}_{site}_{tail_s}\")\n",
        "            \n",
        "        clips = torch.cat(clips, dim=0)\n",
        "        return clips, row_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWAoJNIpeiHn"
      },
      "source": [
        "def birdclef_criterion(outputs, targets):\n",
        "    clipwise_output = outputs[\"clipwise_output\"]\n",
        "    segmentwise_output, _ = outputs[\"segmentwise_output\"].max(2)\n",
        "    loss1 = nn.BCEWithLogitsLoss(reduction=\"mean\")(clipwise_output, targets)\n",
        "    loss2 = nn.BCEWithLogitsLoss(reduction=\"mean\")(segmentwise_output, targets)\n",
        "    loss = loss1 + loss2\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRAaCvsi8n4M"
      },
      "source": [
        "MODEL_HEADER_INFO = {\n",
        "    \"resnet18\": (-2, 512)\n",
        "}\n",
        "\n",
        "def interpolate_and_padding(x, frames_num):  # x: (batch, class_num, time)\n",
        "    ratio = frames_num // x.shape[2]\n",
        "    x = x.transpose(1, 2)  # (batch, time, class_num)\n",
        "    \n",
        "    # interpolate\n",
        "    (batch_size, time_steps, classes_num) = x.shape\n",
        "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
        "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
        "\n",
        "    # padding\n",
        "    output = F.interpolate(\n",
        "        upsampled.unsqueeze(1),\n",
        "        size=(frames_num, upsampled.size(2)),\n",
        "        align_corners=True,\n",
        "        mode=\"bilinear\").squeeze(1)\n",
        "    \n",
        "    output = output.transpose(1, 2) # (batch, class_num, time)\n",
        "    \n",
        "    return output\n",
        "\n",
        "class BirdCLEFNet(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(BirdCLEFNet, self).__init__()\n",
        "        self.model_name = model_name\n",
        "        self.n_label = (n_labels)\n",
        "        \n",
        "        self.mel_spectrogram_extractor = MelSpectrogram(\n",
        "            sample_rate=config.SAMPLE_RATE,\n",
        "            n_fft=config.N_FFT,\n",
        "            f_min=config.FMIN, \n",
        "            f_max=config.FMAX,\n",
        "            n_mels=config.SPEC_HEIGHT,\n",
        "            hop_length=config.HOP_LENGTH,\n",
        "        )\n",
        "        self.amplitude_to_db = AmplitudeToDB()\n",
        "        self.spec_augment_freq = DropStripes(dim=2, drop_width=30, stripes_num=1)\n",
        "        #self.spec_augment_time = DropStripes(dim=3, drop_width=30, stripes_num=1)\n",
        "\n",
        "        base_model = timm.create_model(model_name, pretrained=True, in_chans=1)\n",
        "        h_idx, n_dense = MODEL_HEADER_INFO[model_name]        \n",
        "        self.model_head = nn.Sequential(*list(base_model.children())[:h_idx])\n",
        "                \n",
        "        self.fc_a = nn.Conv1d(n_dense, self.n_label, 1)\n",
        "        self.fc_b = nn.Conv1d(n_dense, self.n_label, 1)\n",
        "\n",
        "    def forward(self, x):  # input x: (batch, Hz, time)\n",
        "        h = x.unsqueeze(1)  # (batch, channel, Hz, time)\n",
        "        h = self.mel_spectrogram_extractor(h)  # (batch, channel, Hz, time)\n",
        "        h = self.amplitude_to_db(h)\n",
        "\n",
        "        if self.training:\n",
        "            #h = self.spec_augment_freq(h)\n",
        "            #h = self.spec_augment_time(h)\n",
        "            pass\n",
        "\n",
        "        frames_num = h.shape[3]\n",
        "        h = self.model_head(h)  # (batch, unit, Hz, time)        \n",
        "        h = F.relu(h)\n",
        "        time_pool = torch.mean(h, dim=2)  # (batch, unit, time)\n",
        "\n",
        "        xa = self.fc_a(time_pool)  # (batch, n_class, time)\n",
        "        xb = self.fc_b(time_pool)  # (batch, n_class, time)\n",
        "        xb = torch.softmax(xb, dim=2)\n",
        "\n",
        "        # time pool\n",
        "        clipwise_output = torch.sum(xa * xb, dim=2)\n",
        "        segmentwise_output = interpolate_and_padding(xa, frames_num)\n",
        "\n",
        "        return {\n",
        "            \"clipwise_output\": clipwise_output,\n",
        "            \"segmentwise_output\": segmentwise_output,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEY4r1lA8wLp"
      },
      "source": [
        "train_metadata_df = pd.read_csv(f\"{config.INPUT_ROOT}/train_metadata.csv\")\n",
        "train_soundscape_labels_df = pd.read_csv(f\"{config.INPUT_ROOT}/train_soundscape_labels.csv\")\n",
        "\n",
        "test_audios = list(Path(f\"{config.INPUT_ROOT}/train_soundscapes/\").glob(\"*.ogg\"))\n",
        "test_dset = TestDataset(test_audios)\n",
        "\n",
        "exist_labels = os.listdir(f\"{config.INPUT_ROOT}\")\n",
        "print(\"original data:\", len(train_metadata_df))\n",
        "train_metadata_df = train_metadata_df.query(f\"primary_label in {exist_labels}\").reset_index(drop=True)\n",
        "print(\"use data:\", len(train_metadata_df))\n",
        "\n",
        "filenames = train_metadata_df[\"filename\"]\n",
        "primary_labels = train_metadata_df[\"primary_label\"]\n",
        "label_dic = {v:i for i, v in enumerate(primary_labels.unique())}\n",
        "label_dic_inv = {i:v for i, v in enumerate(primary_labels.unique())}\n",
        "n_labels = len(label_dic)\n",
        "\n",
        "print(\"### labels ###\")\n",
        "print(label_dic)\n",
        "print(label_dic_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-6L8e1682rc"
      },
      "source": [
        "def train_loop(train_data_loader, model, optimizer, scheduler):\n",
        "    losses, lrs = [], []\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    for n_iter, (X, y) in tqdm_notebook(enumerate(train_data_loader), total=len(train_data_loader)):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        outputs = model(X)\n",
        "        loss = birdclef_criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        \n",
        "        if n_iter % config.N_ACCUMULATE == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        lrs.append(np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean())\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "    return losses, lrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq9K11WN85aP"
      },
      "source": [
        "def valid_loop(valid_data_loader, model):\n",
        "    losses = []\n",
        "    predicts = []\n",
        "    model.eval()\n",
        "    for n_iter, (X, y) in tqdm_notebook(enumerate(valid_data_loader), total=len(valid_data_loader)):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X)\n",
        "        loss = birdclef_criterion(outputs, y)\n",
        "        losses.append(loss.item())\n",
        "        _pred = outputs[\"clipwise_output\"]\n",
        "        #_pred, _ = outputs[\"segmentwise_output\"].max(2)\n",
        "        predicts.append(_pred)\n",
        "    valid_predicts = torch.cat(predicts, dim=0)\n",
        "    return losses, valid_predicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_IIuOgtuOeZ"
      },
      "source": [
        "def test_loop(test_dset, model):\n",
        "    res_dfs = []\n",
        "    model.eval()\n",
        "    for wave, row_ids in tqdm_notebook(test_dset):\n",
        "        X = wave.to(device)        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(X)\n",
        "        _pred = outputs[\"clipwise_output\"].sigmoid().cpu()\n",
        "        #_pred, _ = outputs[\"segmentwise_output\"].sigmoid().cpu().max(2)\n",
        "        predict_labels = output_to_label(_pred, config.THRESHOLD) \n",
        "        res_df = pd.DataFrame(zip(row_ids, predict_labels), columns=[\"row_id\", \"birds\"])\n",
        "        res_dfs.append(res_df)\n",
        "    submission_df = pd.concat(res_dfs, axis=0)\n",
        "\n",
        "    rows = []\n",
        "    for row_id in train_soundscape_labels_df[\"row_id\"]:\n",
        "        row = submission_df.query(f\"row_id=='{row_id}'\")\n",
        "        rows.append(row)\n",
        "    submission_df = pd.concat(rows).reset_index(drop=True)\n",
        "\n",
        "    y_true = train_soundscape_labels_df[\"birds\"].tolist()\n",
        "    y_pred = submission_df[\"birds\"].tolist()\n",
        "    local_score = row_wise_micro_averaged_f1_score(y_true, y_pred)\n",
        "\n",
        "    return local_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4HUy0grQcba"
      },
      "source": [
        "def output_to_label(clipwise_output, thr):\n",
        "    lst = []\n",
        "    for pred in clipwise_output:\n",
        "        pred_labs = [label_dic_inv[i] for i, v in enumerate(pred) if v > thr]\n",
        "        if len(pred_labs) == 0:\n",
        "            pred_labs = \"nocall\"\n",
        "        else:\n",
        "            pred_labs = \" \".join(pred_labs)\n",
        "        lst.append(pred_labs)\n",
        "    return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMLin9dSM7DR"
      },
      "source": [
        "def calc_mAP(valid_primary_labels, valid_predicts):\n",
        "    targets = np.array([label_dic[i] for i in valid_primary_labels])\n",
        "    precisions = []\n",
        "    for lab_i in range(n_labels):\n",
        "        y_true = (targets == lab_i).astype(int)\n",
        "        y_pred = (valid_predicts[:, lab_i] > config.THRESHOLD).numpy().astype(int)\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        precisions.append(precision)\n",
        "    mAP = np.array(precisions).mean()\n",
        "    return mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gLM5sTP8x-9"
      },
      "source": [
        "#%debug\n",
        "skf = StratifiedKFold(n_splits=5,  shuffle=True, random_state=config.SEED)\n",
        "for fold, (train_index, valid_index) in enumerate(skf.split(filenames, primary_labels)):\n",
        "    if fold in [0, 1, 2]:\n",
        "        continue\n",
        "    print(f\"### Fold-{fold} ###\")\n",
        "    set_seed(config.SEED)\n",
        "    outdir = f\"{config.OUTPUT_ROOT}/exp{config.EXP_NUM}_{config.EXP_NAME}\"\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # データセットの準備\n",
        "    train_primary_labels = primary_labels.loc[train_index].values\n",
        "    valid_primary_labels = primary_labels.loc[valid_index].values\n",
        "    train_filenames = filenames.loc[train_index].values \n",
        "    valid_filenames = filenames.loc[valid_index].values\n",
        "    train_dset = BirdCLEFTrainDataset(train_filenames, train_primary_labels, \"train\")\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    valid_dset = BirdCLEFTrainDataset(valid_filenames, valid_primary_labels, \"valid\")\n",
        "    valid_data_loader = torch.utils.data.DataLoader(valid_dset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    # モデル関係\n",
        "    model = BirdCLEFNet(config.MODEL_NAME)\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=len(train_data_loader)*config.T_MAX, eta_min=0.0)\n",
        "\n",
        "    # 学習ログのwatch\n",
        "    uniqe_exp_name = f\"exp{config.EXP_NUM}_freq{config.LABEL_FREQ}_f{fold}_{config.EXP_NAME}\"\n",
        "    wandb.init(project='toda_exp', entity='birdclef', name=uniqe_exp_name)\n",
        "    wandb_config = wandb.config\n",
        "    wandb_config.fold = fold\n",
        "    for k, v in dict(vars(config)).items():\n",
        "        if k[:2] == \"__\":\n",
        "            continue\n",
        "        wandb_config[k] = v\n",
        "    #wandb.watch(model)\n",
        "    \n",
        "    best_f1, best_mAP = 0, 0\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"[{epoch} epoch]\")\n",
        "        train_losses, lrs = train_loop(train_data_loader, model, optimizer, scheduler)\n",
        "        valid_losses, valid_predicts = valid_loop(valid_data_loader, model)\n",
        "        f1_train_soundscape = test_loop(test_dset, model)\n",
        "\n",
        "        valid_predicts = valid_predicts.sigmoid().cpu()\n",
        "    \n",
        "\n",
        "        predict_labels = output_to_label(valid_predicts, config.THRESHOLD)\n",
        "        epoch_f1 = row_wise_micro_averaged_f1_score(valid_primary_labels, predict_labels)\n",
        "        epoch_mAP = calc_mAP(valid_primary_labels, valid_predicts)\n",
        "\n",
        "        if best_f1 < epoch_f1:\n",
        "            best_f1 = epoch_f1\n",
        "            torch.save(model.state_dict(), f\"{outdir}/birdclefnet_f{fold}_f1_best_model.bin\")\n",
        "        if best_mAP < epoch_mAP:\n",
        "            best_mAP = epoch_mAP\n",
        "            torch.save(model.state_dict(), f\"{outdir}/birdclefnet_f{fold}_mAP_best_model.bin\")\n",
        "\n",
        "        res_d = dict()\n",
        "        res_d[\"t_loss\"] = np.array(train_losses).mean()\n",
        "        res_d[\"v_loss\"] = np.array(valid_losses).mean()\n",
        "        res_d[\"lr_avg\"] = np.array(lrs).mean()\n",
        "        res_d[\"epoch_f1\"] = epoch_f1\n",
        "        res_d[\"best_f1\"] = best_f1\n",
        "        res_d[\"epoch_mAP\"] = epoch_mAP\n",
        "        res_d[\"best_mAP\"] = best_mAP\n",
        "        res_d[\"f1_train_soundscape\"] = f1_train_soundscape\n",
        "\n",
        "        wandb.log(res_d)\n",
        "        torch.save(model.state_dict(), f\"{outdir}/birdclefnet_f{fold}_last_model.bin\")\n",
        "\n",
        "    wandb.finish()\n",
        "    # break  # only Fold-0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DOC8GGUQxJa"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlvjxnrMLunQ"
      },
      "source": [
        "train_losses, lrs = train_loop(train_data_loader, model, optimizer, scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmtvf-9S7slF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}