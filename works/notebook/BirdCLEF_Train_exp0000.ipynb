{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BirdCLEF_Train_exp0001.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2kY0qCDAS0-"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7LAfWzBAd-a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JssPm9jrAlV0"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p .kaggle\n",
        "!cp \"./drive/My Drive/Study/config/kaggle.json\" .kaggle/\n",
        "!chmod 600 .kaggle/kaggle.json\n",
        "!mv .kaggle /root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra1HjFKF6Elq"
      },
      "source": [
        "# 6分くらい\n",
        "%%time\n",
        "!mkdir -p birdclef-2021\n",
        "\n",
        "!kaggle competitions download -c birdclef-2021 -f train_metadata.csv\n",
        "!kaggle datasets download takamichitoda/birdclef-split-audio-frequency-500400\n",
        "\n",
        "!unzip birdclef-split-audio-frequency-500400.zip -d birdclef-2021 > /dev/null\n",
        "!unzip train_metadata.csv.zip -d birdclef-2021 > /dev/null\n",
        "\n",
        "!rm birdclef-split-audio-frequency-500400.zip train_metadata.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TueNq39B76ko"
      },
      "source": [
        "!pip install timm torchaudio evaluations wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDb3TFKPMGDA"
      },
      "source": [
        "with open(\"./drive/My Drive/Study/config/wandb.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        wandb_key = line.replace(\"\\n\", \"\")\n",
        "\n",
        "!wandb login {wandb_key}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GHfTaGo7RAy"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import psutil\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import albumentations as A\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import wandb\n",
        "import timm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from evaluations.kaggle_2020 import row_wise_micro_averaged_f1_score\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTW9NsOx7-op"
      },
      "source": [
        "class config:\n",
        "    EXP_NUM = \"0000\"\n",
        "    EXP_NAME = \"baseline\"\n",
        "    # data ssetting\n",
        "    INPUT_ROOT = \"/content/birdclef-2021\"\n",
        "    WORK_ROOT = \"/content\"\n",
        "    OUTPUT_ROOT = \"/content/drive/MyDrive/Study/BirdCLEF/output\"\n",
        "    LABEL_FREQ = \"500-400\"\n",
        "    # audio setting\n",
        "    SAMPLE_RATE = 32000\n",
        "    FMIN = 20\n",
        "    FMAX = 16000\n",
        "    N_FFT = 2048\n",
        "    SPEC_HEIGHT = 128\n",
        "    PERIOD = 20\n",
        "    HOP_LENGTH = 512\n",
        "    # ML setting\n",
        "    SEED = 416\n",
        "    BATCH_SIZE = 64\n",
        "    MODEL_NAME = \"resnet18\"\n",
        "    LEAENING_RATE = 1e-3\n",
        "    T_MAX = 5\n",
        "    NUM_EPOCHS = 30\n",
        "    N_ACCUMULATE = 1\n",
        "    DATA_N_LIMIT = 100\n",
        "    # infer setting\n",
        "    THRESHOLD = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvUvLhMDaVhP"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5QsIqBj8mh0"
      },
      "source": [
        "def arrange_wave_length(waveform):\n",
        "    effective_length = config.PERIOD * config.SAMPLE_RATE\n",
        "    input_length = waveform.shape[1]\n",
        "    if input_length > effective_length:\n",
        "        head_idx = np.random.randint(input_length - effective_length)\n",
        "        _waveform = waveform[:, head_idx:head_idx+effective_length]\n",
        "    elif input_length < effective_length:\n",
        "        pad = torch.zeros((1, effective_length - input_length))\n",
        "        _waveform = torch.hstack([waveform, pad])\n",
        "    else:\n",
        "        _waveform = waveform\n",
        "    return _waveform\n",
        "\n",
        "\n",
        "class BirdCLEFTrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fnames, labels, mode):\n",
        "        self.fnames = fnames\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fnames[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        audio_path = f\"{config.INPUT_ROOT}/{label}/{fname}\"\n",
        "\n",
        "        #waveform, sample_rate = torchaudio.load(audio_path, normalization=True)\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        waveform = arrange_wave_length(waveform)\n",
        "        \n",
        "        label_ohe = torch.eye(n_labels)[label_dic[label]]\n",
        "        \n",
        "        return waveform, label_ohe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWAoJNIpeiHn"
      },
      "source": [
        "def birdclef_criterion(outputs, targets, device):\n",
        "    clipwise_output = outputs[\"clipwise_output\"]\n",
        "    loss = nn.BCEWithLogitsLoss(reduction=\"mean\")(clipwise_output, targets)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRAaCvsi8n4M"
      },
      "source": [
        "MODEL_HEADER_INFO = {\n",
        "    \"resnet18\": (-2, 512)\n",
        "}\n",
        "\n",
        "def interpolate_and_padding(x, frames_num):  # x: (batch, class_num, time)\n",
        "    ratio = frames_num // x.shape[2]\n",
        "    x = x.transpose(1, 2)  # (batch, time, class_num)\n",
        "    \n",
        "    # interpolate\n",
        "    (batch_size, time_steps, classes_num) = x.shape\n",
        "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
        "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
        "\n",
        "    # padding\n",
        "    output = F.interpolate(\n",
        "        upsampled.unsqueeze(1),\n",
        "        size=(frames_num, upsampled.size(2)),\n",
        "        align_corners=True,\n",
        "        mode=\"bilinear\").squeeze(1)\n",
        "    \n",
        "    output = output.transpose(1, 2) # (batch, class_num, time)\n",
        "    \n",
        "    return output\n",
        "\n",
        "class BirdCLEFNet(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(BirdCLEFNet, self).__init__()\n",
        "        self.model_name = model_name\n",
        "        self.n_label = (n_labels)\n",
        "        \n",
        "        self.mel_spectrogram_extractor = MelSpectrogram(\n",
        "            sample_rate=config.SAMPLE_RATE,\n",
        "            n_fft=config.N_FFT,\n",
        "            f_min=config.FMIN, \n",
        "            f_max=config.FMAX,\n",
        "            n_mels=config.SPEC_HEIGHT,\n",
        "            hop_length=config.HOP_LENGTH,\n",
        "        )\n",
        "        self.amplitude_to_db = AmplitudeToDB()\n",
        "\n",
        "        base_model = timm.create_model(model_name, pretrained=True, in_chans=1)\n",
        "        h_idx, n_dense = MODEL_HEADER_INFO[model_name]        \n",
        "        self.model_head = nn.Sequential(*list(base_model.children())[:h_idx])\n",
        "                \n",
        "        self.fc_a = nn.Conv1d(n_dense, self.n_label, 1)\n",
        "        self.fc_b = nn.Conv1d(n_dense, self.n_label, 1)\n",
        "\n",
        "    def forward(self, x):  # input x: (batch, channel, Hz, time)\n",
        "        h = x  # (batch, channel, time)\n",
        "        h = self.mel_spectrogram_extractor(h)  # (batch, channel, Hz, time)\n",
        "        h = self.amplitude_to_db(h)\n",
        "\n",
        "        frames_num = h.shape[3]\n",
        "        h = self.model_head(h)  # (batch, unit, Hz, time)        \n",
        "        h = F.relu(h)\n",
        "        time_pool = torch.mean(h, dim=2)  # (batch, unit, time)\n",
        "\n",
        "        xa = self.fc_a(time_pool)  # (batch, n_class, time)\n",
        "        xb = self.fc_b(time_pool)  # (batch, n_class, time)\n",
        "        xb = torch.softmax(xb, dim=2)\n",
        "\n",
        "        # time pool\n",
        "        clipwise_output = torch.sum(xa * xb, dim=2)\n",
        "        segmentwise_output = interpolate_and_padding(xa, frames_num)\n",
        "\n",
        "        return {\n",
        "            \"clipwise_output\": clipwise_output,\n",
        "            \"segmentwise_output\": segmentwise_output,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEY4r1lA8wLp"
      },
      "source": [
        "train_metadata_df = pd.read_csv(f\"{config.INPUT_ROOT}/train_metadata.csv\")\n",
        "\n",
        "exist_labels = os.listdir(f\"{config.INPUT_ROOT}\")\n",
        "print(\"original data:\", len(train_metadata_df))\n",
        "train_metadata_df = train_metadata_df.query(f\"primary_label in {exist_labels}\").reset_index(drop=True)\n",
        "print(\"use data:\", len(train_metadata_df))\n",
        "\n",
        "filenames = train_metadata_df[\"filename\"]\n",
        "primary_labels = train_metadata_df[\"primary_label\"]\n",
        "label_dic = {v:i for i, v in enumerate(primary_labels.unique())}\n",
        "label_dic_inv = {i:v for i, v in enumerate(primary_labels.unique())}\n",
        "n_labels = len(label_dic)\n",
        "\n",
        "print(\"### labels ###\")\n",
        "print(label_dic)\n",
        "print(label_dic_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-6L8e1682rc"
      },
      "source": [
        "def train_loop(train_data_loader, model, optimizer, scheduler):\n",
        "    losses, lrs = [], []\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    for n_iter, (X, y) in tqdm_notebook(enumerate(train_data_loader), total=len(train_data_loader)):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        outputs = model(X)\n",
        "        loss = birdclef_criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        \n",
        "        if n_iter % config.N_ACCUMULATE == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        lrs.append(np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean())\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "    return losses, lrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq9K11WN85aP"
      },
      "source": [
        "def valid_loop(valid_data_loader, model):\n",
        "    losses = []\n",
        "    predicts = []\n",
        "    model.eval()\n",
        "    for n_iter, (X, y) in tqdm_notebook(enumerate(valid_data_loader), total=len(valid_data_loader)):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X)\n",
        "        loss = birdclef_criterion(outputs, y)\n",
        "        losses.append(loss.item())\n",
        "        clipwise_output = outputs[\"clipwise_output\"]\n",
        "        predicts.append(clipwise_output)\n",
        "    valid_predicts = torch.cat(predicts, dim=0)\n",
        "    return losses, valid_predicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4HUy0grQcba"
      },
      "source": [
        "def output_to_label(clipwise_output, thr):\n",
        "    lst = []\n",
        "    for pred in clipwise_output:\n",
        "        pred_labs = [label_dic_inv[i] for i, v in enumerate(pred) if v > thr]\n",
        "        if len(pred_labs) == 0:\n",
        "            pred_labs = \"nocall\"\n",
        "        else:\n",
        "            pred_labs = \" \".join(pred_labs)\n",
        "        lst.append(pred_labs)\n",
        "    return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gLM5sTP8x-9"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=5,  shuffle=True, random_state=config.SEED)\n",
        "for fold, (train_index, valid_index) in enumerate(skf.split(filenames, primary_labels)):\n",
        "    set_seed(config.SEED)\n",
        "    print(f\"### Fold-{fold} ###\")\n",
        "\n",
        "    # データセットの準備\n",
        "    train_primary_labels = primary_labels.loc[train_index].values\n",
        "    valid_primary_labels = primary_labels.loc[valid_index].values\n",
        "    train_filenames = filenames.loc[train_index].values \n",
        "    valid_filenames = filenames.loc[valid_index].values\n",
        "    train_dset = BirdCLEFTrainDataset(train_filenames, train_primary_labels, \"train\")\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    valid_dset = BirdCLEFTrainDataset(valid_filenames, valid_primary_labels, \"valid\")\n",
        "    valid_data_loader = torch.utils.data.DataLoader(valid_dset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    # モデル関係\n",
        "    model = BirdCLEFNet(config.MODEL_NAME)\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=config.LEAENING_RATE)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=len(train_data_loader)*config.T_MAX, eta_min=0.0)\n",
        "\n",
        "    # 学習ログのwatch\n",
        "    uniqe_exp_name = f\"exp{config.EXP_NUM}_freq{config.LABEL_FREQ}_f{fold}_{config.EXP_NAME}\"\n",
        "    wandb.init(project=\"birdclef\", entity='trtd56', name=uniqe_exp_name)\n",
        "    wandb_config = wandb.config\n",
        "    wandb_config.fold = fold\n",
        "    for k, v in dict(vars(config)).items():\n",
        "        if k[:2] == \"__\":\n",
        "            continue\n",
        "        wandb_config[k] = v\n",
        "    wandb.watch(model)\n",
        "\n",
        "    best_f1 = 0\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        train_losses, lrs = train_loop(train_data_loader, model, optimizer, scheduler)\n",
        "        valid_losses, valid_predicts = valid_loop(valid_data_loader, model)\n",
        "\n",
        "        valid_predicts = valid_predicts.sigmoid().cpu()\n",
        "\n",
        "        predict_labels = output_to_label(valid_predicts, config.THRESHOLD)\n",
        "        epoch_f1 = row_wise_micro_averaged_f1_score(valid_primary_labels, predict_labels)\n",
        "\n",
        "        if best_f1 < epoch_f1:\n",
        "            best_f1 = epoch_f1\n",
        "            torch.save(model.state_dict(), f\"{config.OUTPUT_ROOT}/tmp/birdclefnet_f{fold}_thr05_best_model.bin\")\n",
        "\n",
        "        res_d = dict()\n",
        "        res_d[\"t_loss\"] = np.array(train_losses).mean()\n",
        "        res_d[\"v_loss\"] = np.array(valid_losses).mean()\n",
        "        res_d[\"lr_avg\"] = np.array(lrs).mean()\n",
        "        res_d[\"epoch_f1\"] = epoch_f1\n",
        "        res_d[\"best_f1\"] = best_f1\n",
        "\n",
        "        wandb.log(res_d)\n",
        "        torch.save(model.state_dict(), f\"{config.OUTPUT_ROOT}/tmp/birdclefnet_f{fold}_last_model.bin\")\n",
        "\n",
        "    wandb.finish()\n",
        "    break  # only Fold-0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zobBsAVFUsj2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}